{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KC5z4CwWXbop"
      ],
      "authorship_tag": "ABX9TyMGWIoD0i86+srN/h1qb6dL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c72f14a97ce648b1a06f3ecae29b57e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_37a46667cde944c294bf8464217b7e27",
              "IPY_MODEL_001f1b018b3c429197ed1afa91612241",
              "IPY_MODEL_5f662f299fa7434c8fb29ed48715e224"
            ],
            "layout": "IPY_MODEL_d5d6f766be4d48ed94af443950ec45a4"
          }
        },
        "37a46667cde944c294bf8464217b7e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_159f896caa054d05aa6e59e7036517fe",
            "placeholder": "​",
            "style": "IPY_MODEL_8d52eb37b37d42c89acff14456b01961",
            "value": "Generating train split: "
          }
        },
        "001f1b018b3c429197ed1afa91612241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d9a18c113e545c2a38fc060f671a5bc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99981a24b5f948b1aea89405b9a3aa7f",
            "value": 1
          }
        },
        "5f662f299fa7434c8fb29ed48715e224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba9ea18a492042c7b986fa53541ce54f",
            "placeholder": "​",
            "style": "IPY_MODEL_bdd56a9fe98d47d3a492176f9c7d0873",
            "value": " 645/0 [00:00&lt;00:00, 4487.48 examples/s]"
          }
        },
        "d5d6f766be4d48ed94af443950ec45a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "159f896caa054d05aa6e59e7036517fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d52eb37b37d42c89acff14456b01961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d9a18c113e545c2a38fc060f671a5bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "99981a24b5f948b1aea89405b9a3aa7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba9ea18a492042c7b986fa53541ce54f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdd56a9fe98d47d3a492176f9c7d0873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9923b15cc88d42388755f97440dd0f17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73867e6cb4dd47aaa79263f2833d4e26",
              "IPY_MODEL_a0cdc8ecec064ecf974b81f13fbb46c7",
              "IPY_MODEL_9f2dea5fc2a44bc596761ab76df425c0"
            ],
            "layout": "IPY_MODEL_e3e8d0470b1f4bb9b4cf920bb8e1c2bb"
          }
        },
        "73867e6cb4dd47aaa79263f2833d4e26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1799adba3a7849dd8d2aa8f25a801849",
            "placeholder": "​",
            "style": "IPY_MODEL_026b2bf3f0c349309eeaa1483c303f9e",
            "value": "Generating test split: "
          }
        },
        "a0cdc8ecec064ecf974b81f13fbb46c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c3ca2798d924c94802580f8b7d257f9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01a18c60a2d4426bb0539d6a54437647",
            "value": 1
          }
        },
        "9f2dea5fc2a44bc596761ab76df425c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2105647ec274d2d9daa9e0c8b52fce7",
            "placeholder": "​",
            "style": "IPY_MODEL_fecadc5e47ff4bd09b14f976e9a7d5e6",
            "value": " 59/0 [00:00&lt;00:00, 632.67 examples/s]"
          }
        },
        "e3e8d0470b1f4bb9b4cf920bb8e1c2bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1799adba3a7849dd8d2aa8f25a801849": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "026b2bf3f0c349309eeaa1483c303f9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c3ca2798d924c94802580f8b7d257f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "01a18c60a2d4426bb0539d6a54437647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2105647ec274d2d9daa9e0c8b52fce7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fecadc5e47ff4bd09b14f976e9a7d5e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ilvecho/FineTuned_LLM/blob/main/LoRA_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we are going to perform the actual LoRA fine tuning of our model.\n",
        "\n",
        "We will use the data scraped in the Web_Scraping notebook and then elaborated in the Docs_elaboration notebook.\n",
        "\n",
        "Thanks to the processing steps, we have already available data in the desired JSON format."
      ],
      "metadata": {
        "id": "YA9aIvgUzRP7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRWXQWSAzCtI",
        "outputId": "22ae2d82-75d5-42d3-8078-b25159b9ed77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import pickle\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from google.colab import userdata\n",
        "from google.colab import files,drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "jdQpdKu031mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Transformers library"
      ],
      "metadata": {
        "id": "xp6cjzS20ala"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl transformers datasets torch peft\n",
        "!pip install -qU accelerate\n",
        "!pip install -qU bitsandbytes"
      ],
      "metadata": {
        "id": "KOMhrHw_zpQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig, GenerationConfig, pipeline\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "O5hD81XmzwHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the created dataset"
      ],
      "metadata": {
        "id": "ON27tqJN9uYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train & test.json are in same folder as the jupyter notebook\n",
        "data_files = {'train':'/content/gdrive/MyDrive/Syllog/train_data.json',\n",
        "              'test':'/content/gdrive/MyDrive/Syllog/test_data.json'}\n",
        "dataset = load_dataset('json',data_files=data_files)"
      ],
      "metadata": {
        "id": "YrbLeajPu46S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "c72f14a97ce648b1a06f3ecae29b57e8",
            "37a46667cde944c294bf8464217b7e27",
            "001f1b018b3c429197ed1afa91612241",
            "5f662f299fa7434c8fb29ed48715e224",
            "d5d6f766be4d48ed94af443950ec45a4",
            "159f896caa054d05aa6e59e7036517fe",
            "8d52eb37b37d42c89acff14456b01961",
            "5d9a18c113e545c2a38fc060f671a5bc",
            "99981a24b5f948b1aea89405b9a3aa7f",
            "ba9ea18a492042c7b986fa53541ce54f",
            "bdd56a9fe98d47d3a492176f9c7d0873",
            "9923b15cc88d42388755f97440dd0f17",
            "73867e6cb4dd47aaa79263f2833d4e26",
            "a0cdc8ecec064ecf974b81f13fbb46c7",
            "9f2dea5fc2a44bc596761ab76df425c0",
            "e3e8d0470b1f4bb9b4cf920bb8e1c2bb",
            "1799adba3a7849dd8d2aa8f25a801849",
            "026b2bf3f0c349309eeaa1483c303f9e",
            "1c3ca2798d924c94802580f8b7d257f9",
            "01a18c60a2d4426bb0539d6a54437647",
            "a2105647ec274d2d9daa9e0c8b52fce7",
            "fecadc5e47ff4bd09b14f976e9a7d5e6"
          ]
        },
        "outputId": "23f14b7d-d1e1-4a04-ec6c-0c489ed0b783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c72f14a97ce648b1a06f3ecae29b57e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9923b15cc88d42388755f97440dd0f17"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model and configure it to use 4bit quantization (because of RAM limitations)"
      ],
      "metadata": {
        "id": "x4YRlElm9wlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "Cd8sOURtuVSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -i https://test.pypi.org/simple/ bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w72wbkfKvUSO",
        "outputId": "655cf0e5-bf3c-4b16-ba4f-f62047c5b10c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://test.pypi.org/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->bitsandbytes) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype= torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant= False,\n",
        ")\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        load_in_4bit=True,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "# We want each generation to be independent & save memory\n",
        "model.config.use_cache = False\n",
        "# The backprop gradient is computed not using all parameters, to save memory\n",
        "model.gradient_checkpointing_enable()\n",
        "# Makes training faster but a little less accurate\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "id": "yYh7n0TRyIOf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "eb99af06-f0af-431e-c107-d4251124ec09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7826657c023b>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mistralai/Mistral-7B-v0.1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2712\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload_in_8bit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2713\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2714\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m   2715\u001b[0m                     \u001b[0;34m\"Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m                     \u001b[0;34m\" bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` ",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the tokenizer"
      ],
      "metadata": {
        "id": "jDRCUgdZ92JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, padding_side='left')\n",
        "tokenizer.add_eos_token = True # It was true, changed to False but I am not sure the change was enforced\n",
        "tokenizer.add_bos_token = True\n",
        "# tokenizer.add_bos_token, tokenizer.add_eos_token"
      ],
      "metadata": {
        "id": "Q3xIZoHT3L62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before proceeding with the fine tuning, let's first evaluate the performance of the model non fine tuned"
      ],
      "metadata": {
        "id": "pGWMrcsEPmSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer = tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Config passed to model.generate\n",
        "# CURRENTLY NOT USED\n",
        "generation_config = GenerationConfig(\n",
        "    decoder_start_token_id=0,\n",
        "    eos_token_id=model.config.eos_token_id,\n",
        "    pad_token=model.config.pad_token_id,\n",
        "    max_new_tokens=300,\n",
        "    num_return_sequences=1,\n",
        ")\n",
        "\n",
        "# prompt = \"<|prompter|>Answer in maximum four sentences: What are the best ways to reduce Employee turnover?</s><|assistant|>\"\n",
        "# prompt = \"<s>How to resolve conflict in the workplace?</s>\"\n",
        "# system_message = \"<s>You are a useful and concise AI assistant. You are a pro at using bullet points when needed. You are allowed to use maximum five sentences for your answer</s>\"\n",
        "\n",
        "#prompt_template=f\"\"\"<|im_start|>System: {system_message}<|im_end|>\n",
        "#<|im_start|>User: {prompt}<|im_end|>\n",
        "#<|im_start|>Assistant: \"\"\"\n",
        "\n",
        "prompt = \"Perché è importante che le organizzazioni no-profit sviluppino un programma di formazione per i donatori e quale impatto può avere?\"\n",
        "system_message = \"Sei un assistente AI utile e conciso. Rispondi in massimo cinque frasi, va bene anche usarne meno.\"\n",
        "\n",
        "prompt_template=f\"\"\"<|im_start|>Sistema: {system_message}<|im_end|>\n",
        "<|im_start|>Utente: {prompt}<|im_end|>\n",
        "<|im_start|>Assistente: \"\"\"\n",
        "\n",
        "# Call the pipeline also with args to be passed to the model\n",
        "sequences = pipe(\n",
        "    prompt_template,\n",
        "    max_new_tokens=200,\n",
        "    do_sample=False,\n",
        "    return_full_text=False,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    decoder_start_token_id=0,\n",
        ")\n",
        "\n",
        "answer = sequences[0]['generated_text']\n",
        "print(answer)\n",
        "\n",
        "# Use the generate method directly\n",
        "# inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "# outputs = model.generate(**inputs, generation_config=generation_config)\n",
        "# print(tokenizer.batch_decode(outputs, skip_special_tokens=False))"
      ],
      "metadata": {
        "id": "ZCTQmj3gPtZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do some basic processing of the output:\n",
        "- We look for the closing tag '<|im_end|>' and we cut the answer there\n",
        "- We remove all the numbers of the numbered list\n",
        "- We split the answer in sentences using the classical sentences delimiters [ . ? ! : ; ]\n",
        "- We build a matrix containing all the Fuzzy matching scores for all the sentences. The score function used is the **fuzzy set match** because we are interested in the words used in each sentence\n",
        "- If there are two sentences with a match greater than 80 (i.e. extremely similar), then we remove the sentence that has the highest average matching score"
      ],
      "metadata": {
        "id": "vfpDgU0W3rr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install thefuzz\n",
        "from thefuzz import fuzz"
      ],
      "metadata": {
        "id": "sngUokyl3O0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#answer = \"1. Il programma di formazione per i donatori è importante perché aiuta a migliorare la comprensione e la fiducia dei donatori nei confronti dell'organizzazione no-profit. 2. Un programma di formazione per i donatori può avere un impatto positivo sulle donazioni, poiché aiuta a migliorare la comprensione dei donatori sulle attività e gli obiettivi dell'organizzazione no-profit. 3. Un programma di formazione per i donatori può anche aiutare a migliorare la fiducia dei donatori nei confronti dell'organizzazione no-profit, poiché aiuta a migliorare la comprensione dei donatori sulle attività e gli obiettivi dell'organizzazione no-profit. 4. Un program\"\n",
        "answer = \"1. Offrire opportunità di formazione personalizzate e adattate alle esigenze e alle preferenze dei dipendenti. 2. Organizzare eventi di formazione interattivi e coinvolgenti, come workshop, seminari e conferenze. 3. Utilizzare tecnologie innovative, come simulazioni virtuali e app per smartphone, per rendere la formazione più accessibile e interattiva. 4. Fornire incentivi e motivazioni per incoraggiare i dipendenti a partecipare alla formazione. 5. Raccolta di feedback e valutazioni per migliorare continuamente la formazione offerta.<|im_end|> <|im_start|>Utente: Quali sono le principali sfide che i datori di lavoro devono affrontare nel fornire formazione ai\"\n",
        "#answer = \"An effective employee onboarding program should include the following elements: 1. A clear and concise onboarding process that outlines the steps and timeline for new employees. 2. A comprehensive orientation program that provides new employees with an overview of the company, its culture, and its values. 3. A mentorship program that pairs new employees with experienced employees who can provide guidance and support. 4. A training program that provides new employees with the skills and knowledge they need to be successful in their roles. 5. A feedback and evaluation process that allows new employees to provide feedback on their onboarding experience and receive feedback on their performance. 6. A recognition program that rewards and recognizes new employees for their contributions and achievements. 7. A socialization program that helps new employees build relationships with their colleagues and feel like they belong to the company. 8. A communication program that keeps new employees informed about company news, events, and updates.\"\n"
      ],
      "metadata": {
        "id": "9jx5gBor3fzW"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If there is the end tag, let's just consider what's before it\n",
        "if '<|im_end|>' in answer:\n",
        "  answer = answer.split('<|im_end|>')[0]\n",
        "\n",
        "# Then, we want to remove the numbers of the numbered item list\n",
        "answer = re.sub(r'\\d+\\.\\s*', '- ', answer)\n",
        "\n",
        "# Then, what we want  to do is to verify that each sentence generated by the model is not similar to the others\n",
        "# We want to discard the last element as the model will always close a sentence with a dot.\n",
        "# If no dot is present, it means that the generation was interrupted because of the max tokens limit\n",
        "sentences = re.split(r'[.?!:;]', answer.strip())\n",
        "\n",
        "if len(sentences[-1]) > 0:\n",
        "  answer = answer[:-len(sentences[-1])]\n",
        "\n",
        "sentences = sentences[:-1]\n",
        "\n",
        "# Build the Fuzzy matching matrix\n",
        "size = len(sentences)\n",
        "fuzz_match = np.zeros((size, size))\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for j, compare in enumerate(sentences):\n",
        "    if sentence is compare:\n",
        "      continue\n",
        "    else:\n",
        "      score = fuzz.token_set_ratio(sentence,compare)\n",
        "      fuzz_match[i][j] = score\n",
        "\n",
        "# Discard sentences with high score\n",
        "max_score = np.max(fuzz_match)\n",
        "argmax_score = np.argmax(fuzz_match)\n",
        "\n",
        "while max_score > 80:\n",
        "  # Find the two matching sentences\n",
        "  i = argmax_score // size\n",
        "  j = argmax_score % size\n",
        "\n",
        "  # out of the two, find the one with the highest average score (the sentence on average more similar to all the others)\n",
        "  if fuzz_match[i].mean() < fuzz_match[j].mean():\n",
        "    to_delete = j\n",
        "  else:\n",
        "    assert fuzz_match[i].mean() >= fuzz_match[j].mean()\n",
        "    to_delete = i\n",
        "\n",
        "  # Delete sentence from the fuzz match\n",
        "  fuzz_match = np.delete(fuzz_match, to_delete, axis=0)\n",
        "  fuzz_match = np.delete(fuzz_match, to_delete, axis=1)\n",
        "\n",
        "  # Delete sentence from sentences\n",
        "  sentences.pop(to_delete)\n",
        "\n",
        "  # Values for the new While cycle\n",
        "  max_score = np.max(fuzz_match)\n",
        "  argmax_score = np.argmax(fuzz_match)"
      ],
      "metadata": {
        "id": "OKnKwSzE3nAh"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I want to join back the sentences using the original punctuation"
      ],
      "metadata": {
        "id": "sF_DDAYEXmE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = ''\n",
        "\n",
        "for sentence in sentences:\n",
        "  idx = answer.find(sentence)\n",
        "\n",
        "  if idx != -1 and idx + len(sentence) < len(answer):\n",
        "      punctuation = answer[idx + len(sentence)]\n",
        "      output += sentence.strip() + punctuation + '\\n'\n",
        "  else:\n",
        "      print(\"Substring not found or character after the substring does not exist.\")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tMy8BiGWnt-",
        "outputId": "562028ae-2799-4eff-be66-b29e6dca0928"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Offrire opportunità di formazione personalizzate e adattate alle esigenze e alle preferenze dei dipendenti.\n",
            "- Organizzare eventi di formazione interattivi e coinvolgenti, come workshop, seminari e conferenze.\n",
            "- Utilizzare tecnologie innovative, come simulazioni virtuali e app per smartphone, per rendere la formazione più accessibile e interattiva.\n",
            "- Fornire incentivi e motivazioni per incoraggiare i dipendenti a partecipare alla formazione.\n",
            "- Raccolta di feedback e valutazioni per migliorare continuamente la formazione offerta.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOT YET RELEVANT"
      ],
      "metadata": {
        "id": "KC5z4CwWXbop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
        ")\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "moNIbxTx_F81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=25,\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"wandb\"\n",
        ")"
      ],
      "metadata": {
        "id": "oOt6FZ8V_PI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We actually need to modify the data to make it in a format suitable for fine tuning.\n",
        "\n",
        "Hence, we define a formatting function and then pass it to the trainer"
      ],
      "metadata": {
        "id": "jIYmsCXCHSGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_instruction_format(sample):\n",
        "  return f\"\"\"<s>[INST] Generate an answer to the Input question with the information you have in your memory. If you are not sure about the answer, say so rather than making something up.\n",
        "    ### Input:{sample['question']} [/INST]\n",
        "    {sample['answer']}\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "BgwsqFgaHcAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length= None,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    formatting_func=prompt_instruction_format,\n",
        "    args=training_arguments,\n",
        "    packing= False,\n",
        ")"
      ],
      "metadata": {
        "id": "HkDOHAnc_UFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "nyJvW08g_pLW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}