{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgfPpN54GcnjAIzb63YJ6a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ilvecho/FineTuned_LLM/blob/main/Docs_elaboration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook is to elaborate the text extracted from the web.\n",
        "\n",
        "In particular, to fine-tune in a supervised fashion, we need **prompt - expected answer** pairs.\n",
        "\n",
        "Hence, we leverage existing LLMs (ChatGPT) to create such pairs starting from the text we extracted.\n",
        "\n",
        "Then, further processing is needed, namely:\n",
        "- We need to translate the pairs from English to Italian\n",
        "- We need to reformat the pairs to have it how LoRA likes it"
      ],
      "metadata": {
        "id": "8iVLzL_iLPxl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1WcgcfemK5zN",
        "outputId": "fd1523bc-0b22-4533-8919-a98e8b885d98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from google.colab import userdata\n",
        "from google.colab import files,drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From text to prompt-answer pairs"
      ],
      "metadata": {
        "id": "1Z2sna3kEyPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the needed libraries"
      ],
      "metadata": {
        "id": "V48AYNl5W50s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain"
      ],
      "metadata": {
        "id": "iUWBkDad_LK5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU openai"
      ],
      "metadata": {
        "id": "WuBF7pehNwE6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "import json"
      ],
      "metadata": {
        "id": "O6RCQpy-F0d4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get the API key from the local secure storage"
      ],
      "metadata": {
        "id": "GxRObc3qW8Q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = userdata.get('OpenAI_API_Key')"
      ],
      "metadata": {
        "id": "ZGLlM89sFFTu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt templates"
      ],
      "metadata": {
        "id": "fih-HnvUXASD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI()\n",
        "\n",
        "model_3_5_turbo_1106 = \"gpt-3.5-turbo-1106\"\n",
        "model = model_3_5_turbo_1106\n",
        "\n",
        "template_general_questions = \"\"\"\n",
        "    I provide you with the following context: '''{transcript}'''.\n",
        "    You must identify the general topic that is discussed in the provided context.\n",
        "    Once the general topic is identified, you need to generate 5 pairs of Question-Answer on the general topic.\n",
        "    Since the questions are generic, the answers must be at least 2 sentences (but do not go above 6 sentences).\n",
        "\"\"\"\n",
        "\n",
        "template_specific_questions = \"\"\"\n",
        "    I provide you with the following context: '''{transcript}'''.\n",
        "    You must identify the general topic that is discussed in the provided context.\n",
        "    Once the general topic is identified, one related sub-topic covered in the provided context.\n",
        "    In the output list all the identified sub-topics in a numbered list. You can use it to double check that the identified sub-topics are five.\n",
        "    Create two Question-Answer pair for said sub-topic. Double check that they are two.\n",
        "    Since the question are specific to a sub-topic, the answer must be at most four sentences long.\n",
        "    Repeat the above actions for five different sub-topics covered in the context.\n",
        "    Before providing the output, review your answer and make sure that five sub topics have been identified.\n",
        "\"\"\"\n",
        "\n",
        "content = \"\"\"\n",
        "    You are a helpful assistant that reads documents, understand their content, and generate Question-Answer pairs.\n",
        "    Your output will be used to perform supervised fine tuning of a LLM - keep it in mind when formulating both the question and the answer.\n",
        "    The desired output format is the following:\n",
        "    - The first line of the output should be \"Topic:\" followed by the topic identified in the provided document\n",
        "    - Identify the questions with \"Question:\" and the answers with \"Answer:\"\n",
        "    - each question and each answer need to be in one line only. The result of this is that each line will start either with \"Question:\" or with \"Answer:\"\n",
        "    Avoid referring to any Named Entity in the questions, unless extremely relevant for the document content.\n",
        "    Email addresses and phone numbers are not relevant for me - do not mention them at any time.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wdpEoWtqGyaq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = os.listdir('/content/gdrive/MyDrive/Syllog/transcripts')\n",
        "\n",
        "tot_chars = []\n",
        "\n",
        "for file in file_list:\n",
        "\n",
        "  if 'ITA' in file:\n",
        "    continue\n",
        "\n",
        "  file_path = '/content/gdrive/MyDrive/Syllog/transcripts/' + file\n",
        "  transcript = open(file_path, 'r').read()\n",
        "\n",
        "  general_response = ''\n",
        "  specific_response = ''\n",
        "\n",
        "  general_question = PromptTemplate(\n",
        "              input_variables = [\"transcript\"],\n",
        "              template=template_general_questions #  template_specific_questions\n",
        "          )\n",
        "\n",
        "  query = general_question.format(transcript = transcript)\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "      model=model,\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": content},\n",
        "          {\"role\": \"user\", \"content\": query}\n",
        "      ],\n",
        "      temperature=0.0,\n",
        "  )\n",
        "\n",
        "  general_response = response.choices[0].message.content\n",
        "\n",
        "\n",
        "  if os.path.getsize(file_path) / 1024 > 4.5:  # If the file is long enough, get the specific questions as well\n",
        "\n",
        "    specific_question = PromptTemplate(\n",
        "                input_variables = [\"transcript\"],\n",
        "                template=template_specific_questions   #  template_general_questions\n",
        "            )\n",
        "\n",
        "    query = specific_question.format(transcript = transcript)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": content},\n",
        "            {\"role\": \"user\", \"content\": query}\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "    )\n",
        "\n",
        "    specific_response = response.choices[0].message.content\n",
        "\n",
        "  # Save the generated pairs\n",
        "  with open(f'/content/gdrive/MyDrive/Syllog/QA_pairs/' + file, 'w') as text_file:\n",
        "    print(f\"Saving questions from file: {file}\")\n",
        "    text_file.write(general_response + '\\n\\n\\n' + specific_response)\n",
        "    text_file.close()"
      ],
      "metadata": {
        "id": "5D6k1PKvHEiB",
        "outputId": "db480b3b-69e8-4226-8633-cc7e336acb34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving questions from file: text_01.txt\n",
            "Saving questions from file: text_02.txt\n",
            "Saving questions from file: text_08.txt\n",
            "Saving questions from file: text_07.txt\n",
            "Saving questions from file: text_06.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Uy0nizyXK3m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}