{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1GsaJgewVQTAohjEYwldi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ilvecho/FineTuned_LLM/blob/main/Docs_elaboration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook is to elaborate the text extracted from the web.\n",
        "\n",
        "In particular, to fine-tune in a supervised fashion, we need **prompt - expected answer** pairs.\n",
        "\n",
        "Hence, we leverage existing LLMs (ChatGPT) to create such pairs starting from the text we extracted.\n",
        "\n",
        "Then, further processing is needed, namely:\n",
        "- We need to translate the pairs from English to Italian\n",
        "- We need to reformat the pairs to have it how LoRA likes it"
      ],
      "metadata": {
        "id": "8iVLzL_iLPxl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "1WcgcfemK5zN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "095b2d5a-54d0-4e71-a815-b938a1da240e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import pickle\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from google.colab import userdata\n",
        "from google.colab import files,drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From text to prompt-answer pairs"
      ],
      "metadata": {
        "id": "1Z2sna3kEyPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the needed libraries"
      ],
      "metadata": {
        "id": "V48AYNl5W50s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain"
      ],
      "metadata": {
        "id": "iUWBkDad_LK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU openai"
      ],
      "metadata": {
        "id": "WuBF7pehNwE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "import json"
      ],
      "metadata": {
        "id": "O6RCQpy-F0d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get the API key from the local secure storage"
      ],
      "metadata": {
        "id": "GxRObc3qW8Q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = userdata.get('OpenAI_API_Key')"
      ],
      "metadata": {
        "id": "ZGLlM89sFFTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt templates"
      ],
      "metadata": {
        "id": "fih-HnvUXASD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI()\n",
        "\n",
        "model_3_5_turbo_1106 = \"gpt-3.5-turbo-1106\"\n",
        "model = model_3_5_turbo_1106\n",
        "\n",
        "template_general_questions = \"\"\"\n",
        "    I provide you with the following context: '''{transcript}'''.\n",
        "    You must identify the general topic that is discussed in the provided context.\n",
        "    Once the general topic is identified, you need to generate 5 pairs of Question-Answer on the general topic.\n",
        "    Since the questions are generic, the answers must be at least 2 sentences (but do not go above 6 sentences).\n",
        "\"\"\"\n",
        "\n",
        "template_specific_questions = \"\"\"\n",
        "    I provide you with the following context: '''{transcript}'''.\n",
        "    You must identify the general topic that is discussed in the provided context.\n",
        "    Once the general topic is identified, one related sub-topic covered in the provided context.\n",
        "    In the output list all the identified sub-topics in a numbered list. You can use it to double check that the identified sub-topics are five.\n",
        "    Create two Question-Answer pair for said sub-topic. Double check that they are two.\n",
        "    Since the question are specific to a sub-topic, the answer must be at most four sentences long.\n",
        "    Repeat the above actions for five different sub-topics covered in the context.\n",
        "    Before providing the output, review your answer and make sure that five sub topics have been identified.\n",
        "\"\"\"\n",
        "\n",
        "content = \"\"\"\n",
        "    You are a helpful assistant that reads documents, understand their content, and generate Question-Answer pairs.\n",
        "    Your output will be used to perform supervised fine tuning of a LLM - keep it in mind when formulating both the question and the answer.\n",
        "    The desired output format is the following:\n",
        "    - The first line of the output should be \"Topic:\" followed by the topic identified in the provided document\n",
        "    - Identify the questions with \"Question:\" and the answers with \"Answer:\"\n",
        "    - each question and each answer need to be in one line only. The result of this is that each line will start either with \"Question:\" or with \"Answer:\"\n",
        "    Avoid referring to any Named Entity in the questions, unless extremely relevant for the document content.\n",
        "    Email addresses and phone numbers are not relevant for me - do not mention them at any time.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wdpEoWtqGyaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = os.listdir('/content/gdrive/MyDrive/Syllog/transcripts')\n",
        "\n",
        "tot_chars = []\n",
        "\n",
        "for file in file_list:\n",
        "\n",
        "  if 'ITA' in file:\n",
        "    continue\n",
        "\n",
        "  file_path = '/content/gdrive/MyDrive/Syllog/transcripts/' + file\n",
        "  transcript = open(file_path, 'r').read()\n",
        "\n",
        "  general_response = ''\n",
        "  specific_response = ''\n",
        "\n",
        "  general_question = PromptTemplate(\n",
        "              input_variables = [\"transcript\"],\n",
        "              template=template_general_questions #  template_specific_questions\n",
        "          )\n",
        "\n",
        "  query = general_question.format(transcript = transcript)\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "      model=model,\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": content},\n",
        "          {\"role\": \"user\", \"content\": query}\n",
        "      ],\n",
        "      temperature=0.0,\n",
        "  )\n",
        "\n",
        "  general_response = response.choices[0].message.content\n",
        "\n",
        "\n",
        "  if os.path.getsize(file_path) / 1024 > 4.5:  # If the file is long enough, get the specific questions as well\n",
        "\n",
        "    specific_question = PromptTemplate(\n",
        "                input_variables = [\"transcript\"],\n",
        "                template=template_specific_questions   #  template_general_questions\n",
        "            )\n",
        "\n",
        "    query = specific_question.format(transcript = transcript)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": content},\n",
        "            {\"role\": \"user\", \"content\": query}\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "    )\n",
        "\n",
        "    specific_response = response.choices[0].message.content\n",
        "\n",
        "  # Save the generated pairs\n",
        "  with open(f'/content/gdrive/MyDrive/Syllog/QA_pairs/' + file, 'w') as text_file:\n",
        "    print(f\"Saving questions from file: {file}\")\n",
        "    text_file.write(general_response + '\\n\\n\\n' + specific_response)\n",
        "    text_file.close()"
      ],
      "metadata": {
        "id": "5D6k1PKvHEiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db480b3b-69e8-4226-8633-cc7e336acb34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving questions from file: text_01.txt\n",
            "Saving questions from file: text_02.txt\n",
            "Saving questions from file: text_08.txt\n",
            "Saving questions from file: text_07.txt\n",
            "Saving questions from file: text_06.txt\n",
            "Saving questions from file: text_03.txt\n",
            "Saving questions from file: text_04.txt\n",
            "Saving questions from file: text_05.txt\n",
            "Saving questions from file: text_09.txt\n",
            "Saving questions from file: text_10.txt\n",
            "Saving questions from file: text_23.txt\n",
            "Saving questions from file: text_25.txt\n",
            "Saving questions from file: text_29.txt\n",
            "Saving questions from file: text_32.txt\n",
            "Saving questions from file: text_33.txt\n",
            "Saving questions from file: text_34.txt\n",
            "Saving questions from file: text_37.txt\n",
            "Saving questions from file: text_42.txt\n",
            "Saving questions from file: text_45.txt\n",
            "Saving questions from file: text_46.txt\n",
            "Saving questions from file: text_47.txt\n",
            "Saving questions from file: text_58.txt\n",
            "Saving questions from file: text_59.txt\n",
            "Saving questions from file: text_62.txt\n",
            "Saving questions from file: text_65.txt\n",
            "Saving questions from file: text_67.txt\n",
            "Saving questions from file: text_69.txt\n",
            "Saving questions from file: text_70.txt\n",
            "Saving questions from file: text_80.txt\n",
            "Saving questions from file: text_82.txt\n",
            "Saving questions from file: text_83.txt\n",
            "Saving questions from file: text_85.txt\n",
            "Saving questions from file: text_86.txt\n",
            "Saving questions from file: text_14.txt\n",
            "Saving questions from file: text_11.txt\n",
            "Saving questions from file: text_15.txt\n",
            "Saving questions from file: text_20.txt\n",
            "Saving questions from file: text_12.txt\n",
            "Saving questions from file: text_13.txt\n",
            "Saving questions from file: text_17.txt\n",
            "Saving questions from file: text_18.txt\n",
            "Saving questions from file: text_19.txt\n",
            "Saving questions from file: text_16.txt\n",
            "Saving questions from file: text_21.txt\n",
            "Saving questions from file: text_26.txt\n",
            "Saving questions from file: text_30.txt\n",
            "Saving questions from file: text_24.txt\n",
            "Saving questions from file: text_22.txt\n",
            "Saving questions from file: text_28.txt\n",
            "Saving questions from file: text_27.txt\n",
            "Saving questions from file: text_35.txt\n",
            "Saving questions from file: text_36.txt\n",
            "Saving questions from file: text_44.txt\n",
            "Saving questions from file: text_48.txt\n",
            "Saving questions from file: text_50.txt\n",
            "Saving questions from file: text_49.txt\n",
            "Saving questions from file: text_60.txt\n",
            "Saving questions from file: text_52.txt\n",
            "Saving questions from file: text_64.txt\n",
            "Saving questions from file: text_61.txt\n",
            "Saving questions from file: text_66.txt\n",
            "Saving questions from file: text_63.txt\n",
            "Saving questions from file: text_68.txt\n",
            "Saving questions from file: text_72.txt\n",
            "Saving questions from file: text_74.txt\n",
            "Saving questions from file: text_71.txt\n",
            "Saving questions from file: text_75.txt\n",
            "Saving questions from file: text_77.txt\n",
            "Saving questions from file: text_73.txt\n",
            "Saving questions from file: text_76.txt\n",
            "Saving questions from file: text_78.txt\n",
            "Saving questions from file: text_81.txt\n",
            "Saving questions from file: text_79.txt\n",
            "Saving questions from file: text_87.txt\n",
            "Saving questions from file: text_89.txt\n",
            "Saving questions from file: text_88.txt\n",
            "Saving questions from file: text_84.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translate & save in proper format"
      ],
      "metadata": {
        "id": "jXzxbCfFe6vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U deep-translator"
      ],
      "metadata": {
        "id": "8Uy0nizyXK3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50cbd482-5746-4f15-ff76-d55f8e9005e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deep-translator in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deep_translator import GoogleTranslator"
      ],
      "metadata": {
        "id": "L8NXqfp4fRWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"This is just an example to test the capabilities of the translator. Let's see how it performs\"\n",
        "\n",
        "GoogleTranslator(source='en', target='it').translate(text=example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q_1DYS05faeh",
        "outputId": "d0d9ac68-a0e5-4a95-b88e-8a1972f433f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Questo è solo un esempio per testare le capacità del traduttore. Vediamo come si comporta'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entry = {\n",
        "    'id': '',\n",
        "    'question': '',\n",
        "    'answer': ''\n",
        "}\n",
        "\n",
        "entry_empty = entry.copy()\n",
        "entry_list = []\n",
        "cont = 0\n",
        "question_found = False\n",
        "\n",
        "file_list = os.listdir('/content/gdrive/MyDrive/Syllog/QA_pairs')\n",
        "\n",
        "for file in file_list:\n",
        "\n",
        "  print(f'Starting the analysis of file: {file}')\n",
        "  file_path = '/content/gdrive/MyDrive/Syllog/QA_pairs/' + file\n",
        "  lines = open(file_path, 'r').readlines()\n",
        "\n",
        "  for line in lines:\n",
        "    if line.startswith('Question: '):\n",
        "      entry = entry_empty.copy()\n",
        "      entry['id'] = str(cont)\n",
        "      question = line.split('Question: ')[-1].strip()\n",
        "      question = GoogleTranslator(source='en', target='it').translate(text=question)\n",
        "      entry['question'] = question\n",
        "      question_found = True\n",
        "\n",
        "    elif line.startswith('Answer: ') and question_found:\n",
        "      answer = line.split('Answer: ')[-1].strip()\n",
        "      answer = GoogleTranslator(source='en', target='it').translate(text=answer)\n",
        "      entry['answer'] = answer\n",
        "      entry_list.append(entry)\n",
        "      cont += 1\n",
        "      question_found = False\n",
        "\n",
        "  with open('/content/gdrive/MyDrive/Syllog/data.json', 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(entry_list, json_file, ensure_ascii=False)\n",
        "\n",
        "  print(f'Cont: {cont}')"
      ],
      "metadata": {
        "id": "KnAI-fKultoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Test set"
      ],
      "metadata": {
        "id": "Y0qbPFYsgnx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = open('/content/gdrive/MyDrive/Syllog/text_num_to_ID_count.txt', 'r').readlines()\n",
        "text_to_id = {}\n",
        "\n",
        "i = 0\n",
        "last_id = 0\n",
        "\n",
        "while i < len(lines):\n",
        "  # Make sure we are taking the Text number\n",
        "  assert lines[i].startswith('Starting')\n",
        "  text_num = re.findall(r'\\d+', lines[i])[0]\n",
        "\n",
        "  # Make sure we are taking the Cont number\n",
        "  assert lines[i+1].startswith('Cont')\n",
        "  id_num = int(re.findall(r'\\d+', lines[i+1])[0])\n",
        "\n",
        "  # Save the ID range corresponding to the text\n",
        "  text_to_id[text_num] = (last_id, id_num-1)\n",
        "\n",
        "  # Update the variables\n",
        "  i += 2\n",
        "  last_id = id_num\n",
        "\n",
        "text_to_id"
      ],
      "metadata": {
        "id": "GKzAsHQyhOyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The text randomly selected to be the origin of the test set questions\n",
        "selected_text = [1,2,7,10,11,13,18,19,21,24,27,28,29,32,34,37,44,47,52,59,60,62,66,70,72,75,77,79,81,82,86,88,89]\n",
        "selected_id = []\n",
        "\n",
        "for text in selected_text:\n",
        "\n",
        "\n",
        "  id_range = text_to_id[str(text).zfill(2)]\n",
        "  id = random.randint(id_range[0], id_range[1])\n",
        "  selected_id.append(id)\n",
        "\n",
        "  # If more than 5 questions have been generated from the text, we are taking two questions for the test set\n",
        "  if id_range[1] - id_range[0] > 5:\n",
        "    id_2 = id\n",
        "\n",
        "    # Make sure to take two different samples\n",
        "    while id_2 == id:\n",
        "      id_2 = random.randint(id_range[0], id_range[1])\n",
        "\n",
        "    selected_id.append(id_2)"
      ],
      "metadata": {
        "id": "Ny5zFrAQh-jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/Syllog/selected_id.pkl', 'wb') as file:\n",
        "  pickle.dump(selected_id, file)"
      ],
      "metadata": {
        "id": "BhX3BHxwlKMJ"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/Syllog/data.json', 'r', encoding='utf-8') as json_file:\n",
        "  entry_list = json.load(json_file)\n",
        "\n",
        "test_set = []\n",
        "train_set = entry_list.copy()\n",
        "\n",
        "for id in selected_id:\n",
        "  sample = entry_list[id]\n",
        "  train_set[id] = 'dummy'\n",
        "  test_set.append(sample)\n",
        "\n",
        "# Remove the Dummies we added earlies\n",
        "while 'dummy' in train_set:\n",
        "  train_set.remove('dummy')"
      ],
      "metadata": {
        "id": "PUtRAzv7piAB"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/Syllog/train_data.json', 'w', encoding='utf-8') as json_file:\n",
        "  json.dump(train_set, json_file, ensure_ascii=False)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Syllog/test_data.json', 'w', encoding='utf-8') as json_file:\n",
        "  json.dump(test_set, json_file, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "gWb_2ZJtp2Np"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_set)"
      ],
      "metadata": {
        "id": "SbHU0sRHtO1p",
        "outputId": "2bd0a108-1477-4a22-c345-f245d8b50976",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "645"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_set)"
      ],
      "metadata": {
        "id": "JmaVJtyktQnb",
        "outputId": "75b2ad21-1603-4790-efdd-b37b25b722a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    }
  ]
}