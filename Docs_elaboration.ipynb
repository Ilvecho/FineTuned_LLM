{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyxbsnqV/G4MLNCJ0b/rr5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ilvecho/FineTuned_LLM/blob/main/Docs_elaboration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook is to elaborate the text extracted from the web.\n",
        "\n",
        "In particular, to fine-tune in a supervised fashion, we need **prompt - expected answer** pairs.\n",
        "\n",
        "Hence, we leverage existing LLMs (ChatGPT) to create such pairs starting from the text we extracted.\n",
        "\n",
        "Then, further processing is needed, namely:\n",
        "- We need to translate the pairs from English to Italian\n",
        "- We need to reformat the pairs to have it how LoRA likes it"
      ],
      "metadata": {
        "id": "8iVLzL_iLPxl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1WcgcfemK5zN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97d04b1-7505-422a-a23b-244ef83942b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from google.colab import userdata\n",
        "from google.colab import files,drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From text to prompt-answer pairs"
      ],
      "metadata": {
        "id": "1Z2sna3kEyPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the needed libraries"
      ],
      "metadata": {
        "id": "V48AYNl5W50s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain"
      ],
      "metadata": {
        "id": "iUWBkDad_LK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU openai"
      ],
      "metadata": {
        "id": "WuBF7pehNwE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "import json"
      ],
      "metadata": {
        "id": "O6RCQpy-F0d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get the API key from the local secure storage"
      ],
      "metadata": {
        "id": "GxRObc3qW8Q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = userdata.get('OpenAI_API_Key')"
      ],
      "metadata": {
        "id": "ZGLlM89sFFTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt templates"
      ],
      "metadata": {
        "id": "fih-HnvUXASD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI()\n",
        "\n",
        "model_3_5_turbo_1106 = \"gpt-3.5-turbo-1106\"\n",
        "model = model_3_5_turbo_1106\n",
        "\n",
        "template_general_questions = \"\"\"\n",
        "    I provide you with the following context: '''{transcript}'''.\n",
        "    You must identify the general topic that is discussed in the provided context.\n",
        "    Once the general topic is identified, you need to generate 5 pairs of Question-Answer on the general topic.\n",
        "    Since the questions are generic, the answers must be at least 2 sentences (but do not go above 6 sentences).\n",
        "\"\"\"\n",
        "\n",
        "template_specific_questions = \"\"\"\n",
        "    I provide you with the following context: '''{transcript}'''.\n",
        "    You must identify the general topic that is discussed in the provided context.\n",
        "    Once the general topic is identified, one related sub-topic covered in the provided context.\n",
        "    In the output list all the identified sub-topics in a numbered list. You can use it to double check that the identified sub-topics are five.\n",
        "    Create two Question-Answer pair for said sub-topic. Double check that they are two.\n",
        "    Since the question are specific to a sub-topic, the answer must be at most four sentences long.\n",
        "    Repeat the above actions for five different sub-topics covered in the context.\n",
        "    Before providing the output, review your answer and make sure that five sub topics have been identified.\n",
        "\"\"\"\n",
        "\n",
        "content = \"\"\"\n",
        "    You are a helpful assistant that reads documents, understand their content, and generate Question-Answer pairs.\n",
        "    Your output will be used to perform supervised fine tuning of a LLM - keep it in mind when formulating both the question and the answer.\n",
        "    The desired output format is the following:\n",
        "    - The first line of the output should be \"Topic:\" followed by the topic identified in the provided document\n",
        "    - Identify the questions with \"Question:\" and the answers with \"Answer:\"\n",
        "    - each question and each answer need to be in one line only. The result of this is that each line will start either with \"Question:\" or with \"Answer:\"\n",
        "    Avoid referring to any Named Entity in the questions, unless extremely relevant for the document content.\n",
        "    Email addresses and phone numbers are not relevant for me - do not mention them at any time.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wdpEoWtqGyaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = os.listdir('/content/gdrive/MyDrive/Syllog/transcripts')\n",
        "\n",
        "tot_chars = []\n",
        "\n",
        "for file in file_list:\n",
        "\n",
        "  if 'ITA' in file:\n",
        "    continue\n",
        "\n",
        "  file_path = '/content/gdrive/MyDrive/Syllog/transcripts/' + file\n",
        "  transcript = open(file_path, 'r').read()\n",
        "\n",
        "  general_response = ''\n",
        "  specific_response = ''\n",
        "\n",
        "  general_question = PromptTemplate(\n",
        "              input_variables = [\"transcript\"],\n",
        "              template=template_general_questions #  template_specific_questions\n",
        "          )\n",
        "\n",
        "  query = general_question.format(transcript = transcript)\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "      model=model,\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": content},\n",
        "          {\"role\": \"user\", \"content\": query}\n",
        "      ],\n",
        "      temperature=0.0,\n",
        "  )\n",
        "\n",
        "  general_response = response.choices[0].message.content\n",
        "\n",
        "\n",
        "  if os.path.getsize(file_path) / 1024 > 4.5:  # If the file is long enough, get the specific questions as well\n",
        "\n",
        "    specific_question = PromptTemplate(\n",
        "                input_variables = [\"transcript\"],\n",
        "                template=template_specific_questions   #  template_general_questions\n",
        "            )\n",
        "\n",
        "    query = specific_question.format(transcript = transcript)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": content},\n",
        "            {\"role\": \"user\", \"content\": query}\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "    )\n",
        "\n",
        "    specific_response = response.choices[0].message.content\n",
        "\n",
        "  # Save the generated pairs\n",
        "  with open(f'/content/gdrive/MyDrive/Syllog/QA_pairs/' + file, 'w') as text_file:\n",
        "    print(f\"Saving questions from file: {file}\")\n",
        "    text_file.write(general_response + '\\n\\n\\n' + specific_response)\n",
        "    text_file.close()"
      ],
      "metadata": {
        "id": "5D6k1PKvHEiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db480b3b-69e8-4226-8633-cc7e336acb34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving questions from file: text_01.txt\n",
            "Saving questions from file: text_02.txt\n",
            "Saving questions from file: text_08.txt\n",
            "Saving questions from file: text_07.txt\n",
            "Saving questions from file: text_06.txt\n",
            "Saving questions from file: text_03.txt\n",
            "Saving questions from file: text_04.txt\n",
            "Saving questions from file: text_05.txt\n",
            "Saving questions from file: text_09.txt\n",
            "Saving questions from file: text_10.txt\n",
            "Saving questions from file: text_23.txt\n",
            "Saving questions from file: text_25.txt\n",
            "Saving questions from file: text_29.txt\n",
            "Saving questions from file: text_32.txt\n",
            "Saving questions from file: text_33.txt\n",
            "Saving questions from file: text_34.txt\n",
            "Saving questions from file: text_37.txt\n",
            "Saving questions from file: text_42.txt\n",
            "Saving questions from file: text_45.txt\n",
            "Saving questions from file: text_46.txt\n",
            "Saving questions from file: text_47.txt\n",
            "Saving questions from file: text_58.txt\n",
            "Saving questions from file: text_59.txt\n",
            "Saving questions from file: text_62.txt\n",
            "Saving questions from file: text_65.txt\n",
            "Saving questions from file: text_67.txt\n",
            "Saving questions from file: text_69.txt\n",
            "Saving questions from file: text_70.txt\n",
            "Saving questions from file: text_80.txt\n",
            "Saving questions from file: text_82.txt\n",
            "Saving questions from file: text_83.txt\n",
            "Saving questions from file: text_85.txt\n",
            "Saving questions from file: text_86.txt\n",
            "Saving questions from file: text_14.txt\n",
            "Saving questions from file: text_11.txt\n",
            "Saving questions from file: text_15.txt\n",
            "Saving questions from file: text_20.txt\n",
            "Saving questions from file: text_12.txt\n",
            "Saving questions from file: text_13.txt\n",
            "Saving questions from file: text_17.txt\n",
            "Saving questions from file: text_18.txt\n",
            "Saving questions from file: text_19.txt\n",
            "Saving questions from file: text_16.txt\n",
            "Saving questions from file: text_21.txt\n",
            "Saving questions from file: text_26.txt\n",
            "Saving questions from file: text_30.txt\n",
            "Saving questions from file: text_24.txt\n",
            "Saving questions from file: text_22.txt\n",
            "Saving questions from file: text_28.txt\n",
            "Saving questions from file: text_27.txt\n",
            "Saving questions from file: text_35.txt\n",
            "Saving questions from file: text_36.txt\n",
            "Saving questions from file: text_44.txt\n",
            "Saving questions from file: text_48.txt\n",
            "Saving questions from file: text_50.txt\n",
            "Saving questions from file: text_49.txt\n",
            "Saving questions from file: text_60.txt\n",
            "Saving questions from file: text_52.txt\n",
            "Saving questions from file: text_64.txt\n",
            "Saving questions from file: text_61.txt\n",
            "Saving questions from file: text_66.txt\n",
            "Saving questions from file: text_63.txt\n",
            "Saving questions from file: text_68.txt\n",
            "Saving questions from file: text_72.txt\n",
            "Saving questions from file: text_74.txt\n",
            "Saving questions from file: text_71.txt\n",
            "Saving questions from file: text_75.txt\n",
            "Saving questions from file: text_77.txt\n",
            "Saving questions from file: text_73.txt\n",
            "Saving questions from file: text_76.txt\n",
            "Saving questions from file: text_78.txt\n",
            "Saving questions from file: text_81.txt\n",
            "Saving questions from file: text_79.txt\n",
            "Saving questions from file: text_87.txt\n",
            "Saving questions from file: text_89.txt\n",
            "Saving questions from file: text_88.txt\n",
            "Saving questions from file: text_84.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translate & save in proper format"
      ],
      "metadata": {
        "id": "jXzxbCfFe6vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U deep-translator"
      ],
      "metadata": {
        "id": "8Uy0nizyXK3m",
        "outputId": "eb7ab960-72c7-4598-a557-4f869aba6f9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m700.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2023.11.17)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deep_translator import GoogleTranslator"
      ],
      "metadata": {
        "id": "L8NXqfp4fRWE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"This is just an example to test the capabilities of the translator. Let's see how it performs\"\n",
        "\n",
        "GoogleTranslator(source='en', target='it').translate(text=example)"
      ],
      "metadata": {
        "id": "Q_1DYS05faeh",
        "outputId": "d0d9ac68-a0e5-4a95-b88e-8a1972f433f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Questo è solo un esempio per testare le capacità del traduttore. Vediamo come si comporta'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2MRScKdfugz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}