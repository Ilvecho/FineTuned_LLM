{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPBttZ9Jl6Sv6oILiAx8Bfu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ilvecho/LLM_fine_tuning/blob/main/Generation_with_Tuned_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook allows you to run the LoRA fine tuned model by Syllog directly on Google Colab.\n",
        "\n",
        "Please use the **T4 GPU** runtime accelerator"
      ],
      "metadata": {
        "id": "6e93X-NMzzeR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and import libraries"
      ],
      "metadata": {
        "id": "PS-XvKqG2Y4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl transformers datasets torch peft\n",
        "!pip install -qU accelerate\n",
        "!pip install -qU bitsandbytes\n",
        "!pip install thefuzz"
      ],
      "metadata": {
        "id": "00LU2e610Tzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_FjCVfKOzvOh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from peft import AutoPeftModelForCausalLM, PeftConfig, PeftModel\n",
        "from thefuzz import fuzz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "7ftLq9RP4kn1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the configuration"
      ],
      "metadata": {
        "id": "wtWOQWp32czy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype= torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant= False,\n",
        ")"
      ],
      "metadata": {
        "id": "Y3YfIBZV1Ni8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is possible to load only from local or from Hugging Face.\n",
        "Hence, we need to create a HF profile"
      ],
      "metadata": {
        "id": "tm_tEe_H5hpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PEFT_MODEL = 'github.com/Ilvecho/LLM_fine_tuning/tree/main/tuned_model'\n",
        "\n",
        "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    return_dict=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, PEFT_MODEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "5wgmjgmg0pDc",
        "outputId": "82c6deda-4498-4ef5-96f8-d4221e07cdd5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Can't find 'adapter_config.json' at 'github.com/Ilvecho/LLM_fine_tuning/tree/main/tuned_model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    144\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhf_hub_download_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'github.com/Ilvecho/LLM_fine_tuning/tree/main/tuned_model'. Use `repo_type` argument if needed.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-16695d9bc1c1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mPEFT_MODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'github.com/Ilvecho/LLM_fine_tuning/tree/main/tuned_model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPEFT_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m model = AutoModelForCausalLM.from_pretrained(\n\u001b[1;32m      5\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 )\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find '{CONFIG_NAME}' at '{pretrained_model_name_or_path}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mloaded_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'github.com/Ilvecho/LLM_fine_tuning/tree/main/tuned_model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer = tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "t1j8xSTX0rCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "#############     GENERATION     #############\n",
        "##############################################\n",
        "\n",
        "system_message = \"Sei un assistente AI utile e conciso. Rispondi in massimo cinque frasi, va bene anche usarne meno.\"\n",
        "\n",
        "prompt_template=f\"\"\"<|im_start|>Sistema: {system_message}<|im_end|>\n",
        "<|im_start|>Utente: {row['question']}<|im_end|>\n",
        "<|im_start|>Assistente: \"\"\"\n",
        "\n",
        "# Call the pipeline also with args to be passed to the model\n",
        "sequences = pipe(\n",
        "    prompt_template,\n",
        "    max_new_tokens=200,\n",
        "    do_sample=False,\n",
        "    return_full_text=False,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    decoder_start_token_id=0,\n",
        ")\n",
        "\n",
        "answer = sequences[0]['generated_text']\n",
        "\n",
        "##############################################\n",
        "#############     PROCESSING     #############\n",
        "##############################################\n",
        "\n",
        "# If there is the end tag, let's just consider what's before it\n",
        "if '<|im_end|>' in answer:\n",
        "  answer = answer.split('<|im_end|>')[0]\n",
        "\n",
        "# Then, we want to remove the numbers of the numbered item list\n",
        "answer = re.sub(r'\\d+\\.\\s*', '- ', answer)\n",
        "\n",
        "# Then, what we want  to do is to verify that each sentence generated by the model is not similar to the others\n",
        "# We want to discard the last element as the model will always close a sentence with a dot.\n",
        "# If no dot is present, it means that the generation was interrupted because of the max tokens limit\n",
        "sentences = re.split(r'[.?!:;]', answer.strip())\n",
        "\n",
        "if len(sentences[-1]) > 0:\n",
        "  answer = answer[:-len(sentences[-1])]\n",
        "\n",
        "# If there are multiple sentences, check that they are different from each other\n",
        "if len(sentences) > 1:\n",
        "  sentences = sentences[:-1]\n",
        "\n",
        "  # Build the Fuzzy matching matrix\n",
        "  size = len(sentences)\n",
        "  fuzz_match = np.zeros((size, size))\n",
        "\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    for j, compare in enumerate(sentences):\n",
        "      if sentence is compare:\n",
        "        continue\n",
        "      else:\n",
        "        score = fuzz.token_set_ratio(sentence,compare)\n",
        "        fuzz_match[i][j] = score\n",
        "\n",
        "  # Discard sentences with high score\n",
        "  max_score = np.max(fuzz_match)\n",
        "  argmax_score = np.argmax(fuzz_match)\n",
        "\n",
        "  while max_score > 80:\n",
        "    # Find the two matching sentences\n",
        "    i = argmax_score // size\n",
        "    j = argmax_score % size\n",
        "\n",
        "    # print(f'Size: {size}, argmax: {argmax_score}, i: {i}, j: {j}')\n",
        "\n",
        "    # out of the two, find the one with the highest average score (the sentence on average more similar to all the others)\n",
        "    if fuzz_match[i].mean() < fuzz_match[j].mean():\n",
        "      to_delete = j\n",
        "    else:\n",
        "      assert fuzz_match[i].mean() >= fuzz_match[j].mean()\n",
        "      to_delete = i\n",
        "\n",
        "    # Delete sentence from the fuzz match\n",
        "    fuzz_match = np.delete(fuzz_match, to_delete, axis=0)\n",
        "    fuzz_match = np.delete(fuzz_match, to_delete, axis=1)\n",
        "\n",
        "    # Since we are deleting one sentence, we need to reduce the size as well\n",
        "    size -= 1\n",
        "\n",
        "    # Delete sentence from sentences\n",
        "    sentences.pop(to_delete)\n",
        "\n",
        "\n",
        "    # Values for the new While cycle\n",
        "    max_score = np.max(fuzz_match)\n",
        "    argmax_score = np.argmax(fuzz_match)\n",
        "\n",
        "  output = ''\n",
        "\n",
        "  for sentence in sentences:\n",
        "    idx = answer.find(sentence)\n",
        "\n",
        "    if idx != -1 and idx + len(sentence) < len(answer):\n",
        "        punctuation = answer[idx + len(sentence)]\n",
        "        output += sentence.strip() + punctuation + '\\n'\n",
        "    else:\n",
        "        print(\"Substring not found or character after the substring does not exist.\")\n",
        "\n",
        "else:\n",
        "  assert len(sentences) == 1\n",
        "  output = sentences[0]\n",
        "\n",
        "assert row['id'] in answers_dict.keys()\n",
        "\n",
        "answers_dict[row['id']]['answer_tuned_model'] = output\n",
        "\n",
        "print(f'Question {cont}: ', row['question'])\n",
        "print(f'Generated output: {output}')\n",
        "# print(f'Reference output: ', row['answer'])\n",
        "print('###################################\\n')"
      ],
      "metadata": {
        "id": "pE2wwz2R09FK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}